{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AaOWkXiHoN4H"
      },
      "source": [
        "# **Master** **Of** **Data** **Science** **NoteBook**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RoQ303Bloa9n"
      },
      "source": [
        "## Analyzing the dataset for my research on : **Preserving** **and reviving the Oshiwambo language through hybrid SVM-Deep Learning Models**, using Python Jupyter notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### Architecture Flow Chart for Hybrid Model:\n",
        "\n",
        "1. **Data Input**:\n",
        "   - Input: Raw text data from multiple columns (e.g., `Oshiwambo`, `Aa-ndonga`, etc.)\n",
        "   \n",
        "2. **Data Cleaning**:\n",
        "   - Tokenization\n",
        "   - Punctuation Removal\n",
        "   - Stemming (applied only to the `Oshiwambo` column)\n",
        "   - Output: Cleaned tokenized text data\n",
        "   \n",
        "3. **One-Hot Encoding** / **Term Frequency-Inverse Document Frequency**/ **Word embending**:\n",
        "   - Convert cleaned text into numerical format (One-Hot Encoding)\n",
        "   - Output: Encoded feature matrix\n",
        "\n",
        "4. **Feature Extraction**:\n",
        "   - **Path 1: LSTM Model**:\n",
        "     - LSTM is used to extract sequential patterns from the encoded data.\n",
        "     - Output: LSTM feature vector\n",
        "   - **Path 2: CNN Model**:\n",
        "     - CNN is used to capture local features from the encoded text data.\n",
        "     - Output: CNN feature vector\n",
        "\n",
        "5. **SVM Classification**:\n",
        "   - SVM is trained using features from LSTM/CNN to categorize language patterns.\n",
        "   - Output: Classified language patterns (e.g., dialects)\n",
        "\n",
        "6. **Model Fusion**:\n",
        "   - Combine the outputs of the SVM and LSTM/CNN models.\n",
        "   - Output: Final prediction based on the fusion of both models.\n",
        "\n",
        "### Labels:\n",
        "- Data Cleaning â†’ Preprocessing â†’ Feature Extraction â†’ Classification â†’ Model Fusion â†’ Final Output\n"
      ],
      "metadata": {
        "id": "YKud1AFTg72D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 1: Load necessary libraries and your CSV file**"
      ],
      "metadata": {
        "id": "dD1sMHSWszQb"
      }
    },
    {
      "source": [
        "# Step 1: Load necessary libraries and CSV file\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "from tensorflow.keras.preprocessing.text import one_hot\n",
        "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
        "import string\n",
        "import nltk\n",
        "\n",
        "# Download necessary NLTK data packages\n",
        "nltk.download('punkt')  # For word tokenization\n",
        "nltk.download('punkt_tab')  # For sentence tokenization (needed by word_tokenize) # This line is the fix\n",
        "\n",
        "# Load your CSV file\n",
        "file_path = '/content/sample_data/Thesis_Dataset - Sheet(11).CSV'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Strip any extra spaces from the column names\n",
        "df.columns = df.columns.str.strip()\n",
        "\n",
        "# Display the column names\n",
        "print(\"Columns in DataFrame after stripping spaces:\")\n",
        "print(df.columns)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Mig7rn74LaW",
        "outputId": "a3a9491b-0add-4142-ac9a-dce5f4d9842d"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns in DataFrame after stripping spaces:\n",
            "Index(['Oshiwambo', 'Aa-ndonga', 'Aa-kwambi', 'Aa-mbalanhu', 'Aa-kwaluudhi',\n",
            "       'Aa-kwanyama', 'Aa-ngandjera', 'Aa-mbandja'],\n",
            "      dtype='object')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# List of columns to be processed\n",
        "columns_to_process = [\n",
        "    'Oshiwambo', 'Aa-ndonga', 'Aa-kwambi', 'Aa-mbalanhu', 'Aa-kwaluudhi', 'Aa-kwanyama', 'Aa-ngandjera', 'Aa-mbandja']\n",
        "# Function to tokenize and remove punctuation (no stemming)\n",
        "def clean_text(text):\n",
        "    # Tokenize the text\n",
        "    tokens = word_tokenize(text)\n",
        "    # Remove punctuation and non-alphabetic tokens\n",
        "    tokens = [word for word in tokens if word.isalpha()]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Apply the cleaning function (tokenize and remove punctuation) to each specified column\n",
        "for column in columns_to_process:\n",
        "    if column in df.columns:\n",
        "        df[column] = df[column].astype(str).apply(clean_text)\n",
        "    else:\n",
        "        print(f\"Column '{column}' not found in DataFrame.\")\n",
        "\n",
        "# Display the first few rows of the cleaned DataFrame (without stemming)\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3bDlKUTpfQ1y",
        "outputId": "605f81e3-4ded-4303-973a-941c6c23b1a1"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Oshiwambo Aa-ndonga  Aa-kwambi Aa-mbalanhu Aa-kwaluudhi Aa-kwanyama  \\\n",
            "0       Ame     Ngame      Ngaye        Aame         Amee         Ame   \n",
            "1       Ove     Ngoye      Ngwee         Oye          Oye         Ove   \n",
            "2                  Ye         Ye          Ye           Ye          Ye   \n",
            "3      Fyee       Tse         Se          Se           Se         Fye   \n",
            "4    Amushe        Ne  Ne amushe         Nye        Amuhe         Nye   \n",
            "\n",
            "  Aa-ngandjera Aa-mbandja  \n",
            "0        Ngaye        Ame  \n",
            "1        Ngwee        ove  \n",
            "2           Ye             \n",
            "3          Tse             \n",
            "4           Ne             \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 2: Apply stemming to the \"Oshiwambo\" column**"
      ],
      "metadata": {
        "id": "Yf0nQiK9tS57"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply stemming to the \"Oshiwambo\" column and replace the original column with stemmed text\n",
        "# Import necessary libraries\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "import pandas as pd\n",
        "\n",
        "# Initialize the Porter Stemmer\n",
        "porter = PorterStemmer()\n",
        "\n",
        "# Function to stem words in the \"Oshiwambo\" column\n",
        "def stem_words(text):\n",
        "    # Assume the text is already tokenized, just apply stemming\n",
        "    stemmed = [porter.stem(word) for word in text.split()]  # Apply stemming to tokenized words\n",
        "    return ' '.join(stemmed)\n",
        "\n",
        "# Apply stemming to the \"Oshiwambo\" column and replace the original column with stemmed text\n",
        "df['Oshiwambo'] = df['Oshiwambo'].astype(str).apply(stem_words)\n",
        "\n",
        "# Display the first few rows of the DataFrame with the replaced \"Oshiwambo\" column\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b908nmOldQAK",
        "outputId": "09e24d2c-677f-4cb4-ce58-d7e1ae130ffc"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Oshiwambo Aa-ndonga  Aa-kwambi Aa-mbalanhu Aa-kwaluudhi Aa-kwanyama  \\\n",
            "0       ame     Ngame      Ngaye        Aame         Amee         Ame   \n",
            "1       ove     Ngoye      Ngwee         Oye          Oye         Ove   \n",
            "2                  Ye         Ye          Ye           Ye          Ye   \n",
            "3      fyee       Tse         Se          Se           Se         Fye   \n",
            "4     amush        Ne  Ne amushe         Nye        Amuhe         Nye   \n",
            "\n",
            "  Aa-ngandjera Aa-mbandja  \n",
            "0        Ngaye        Ame  \n",
            "1        Ngwee        ove  \n",
            "2           Ye             \n",
            "3          Tse             \n",
            "4           Ne             \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 3: One-Hot Encoding**"
      ],
      "metadata": {
        "id": "io5_zc3T7bvP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: One-Hot Encoding\n",
        "from tensorflow.keras.preprocessing.text import one_hot\n",
        "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
        "\n",
        "# List of columns to be one-hot encoded\n",
        "columns_to_encode =  [\n",
        "    'Oshiwambo', 'Aa-ndonga', 'Aa-kwambi', 'Aa-mbalanhu', 'Aa-kwaluudhi', 'Aa-kwanyama', 'Aa-ngandjera', 'Aa-mbandja']\n",
        "\n",
        "# Initialize a set to store unique words across all columns\n",
        "unique_words = set()\n",
        "\n",
        "# Extract unique words from each column\n",
        "for column in columns_to_encode:\n",
        "    df[column] = df[column].astype(str)\n",
        "    for text in df[column]:\n",
        "        words_in_text = text_to_word_sequence(text)\n",
        "        unique_words.update(words_in_text)\n",
        "\n",
        "# Estimate the vocabulary size\n",
        "vocab_size = len(unique_words)\n",
        "print(f\"Vocabulary Size: {vocab_size}\")\n",
        "\n",
        "# One-hot encode each column\n",
        "encoded_columns = {}\n",
        "for column in columns_to_encode:\n",
        "    encoded_column = [one_hot(text, round(vocab_size * 1.3)) for text in df[column]]\n",
        "    encoded_columns[column] = encoded_column\n",
        "\n",
        "# Convert the encoded columns back into a DataFrame\n",
        "encoded_df = pd.DataFrame(encoded_columns)\n",
        "\n",
        "# Display the first few rows of the one-hot encoded DataFrame\n",
        "print(encoded_df.head())\n"
      ],
      "metadata": {
        "id": "nTUex2quspaV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1035cd1-8876-49ea-9f95-b972c1190832"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary Size: 1681\n",
            "  Oshiwambo Aa-ndonga    Aa-kwambi Aa-mbalanhu Aa-kwaluudhi Aa-kwanyama  \\\n",
            "0    [1808]    [1697]       [2066]       [937]       [1020]      [1808]   \n",
            "1     [828]    [1185]       [1014]      [2154]       [2154]       [828]   \n",
            "2        []    [1723]       [1723]      [1723]       [1723]      [1723]   \n",
            "3     [805]    [1735]       [1785]      [1785]       [1785]      [1880]   \n",
            "4     [326]     [257]  [257, 2086]      [1650]        [490]      [1650]   \n",
            "\n",
            "  Aa-ngandjera Aa-mbandja  \n",
            "0       [2066]     [1808]  \n",
            "1       [1014]      [828]  \n",
            "2       [1723]         []  \n",
            "3       [1735]         []  \n",
            "4        [257]         []  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Using TF-IDF Encoding (Term Frequency-Inverse Document Frequency)**\n",
        "\n",
        "Why use this?\n",
        "\n",
        "TF-IDF assigns more importance to meaningful words and reduces the impact of frequently occurring words like \"the\" or \"and.\"\n",
        "\n",
        "It works well for traditional machine learning models like SVM."
      ],
      "metadata": {
        "id": "H-Syt772j8pj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Combine text from all columns\n",
        "all_text = df[columns_to_encode].astype(str).apply(lambda x: ' '.join(x), axis=1)\n",
        "\n",
        "# Initialize TF-IDF vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=5000)  # Limit feature space to 5000 words\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(all_text)\n",
        "\n",
        "# Get the vocabulary size\n",
        "vocab_size = len(tfidf_vectorizer.vocabulary_)\n",
        "print(f\"Vocabulary Size: {vocab_size}\")\n",
        "\n",
        "# Convert to DataFrame\n",
        "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
        "\n",
        "# Display the first few rows\n",
        "print(tfidf_df.head())\n"
      ],
      "metadata": {
        "id": "KeFkFIvU5AYf",
        "outputId": "2123c318-5123-45df-fa14-e1da069b769e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary Size: 1680\n",
            "   aalongwa      aame  aamushanga  aamwahinathana  aamwaina  aamwainafana  \\\n",
            "0       0.0  0.259776         0.0             0.0       0.0           0.0   \n",
            "1       0.0  0.000000         0.0             0.0       0.0           0.0   \n",
            "2       0.0  0.000000         0.0             0.0       0.0           0.0   \n",
            "3       0.0  0.000000         0.0             0.0       0.0           0.0   \n",
            "4       0.0  0.000000         0.0             0.0       0.0           0.0   \n",
            "\n",
            "   aamwayinafana  aamwayinathana  aanasikola  aanona  ...  yomepya  yomomeva  \\\n",
            "0            0.0             0.0         0.0     0.0  ...      0.0       0.0   \n",
            "1            0.0             0.0         0.0     0.0  ...      0.0       0.0   \n",
            "2            0.0             0.0         0.0     0.0  ...      0.0       0.0   \n",
            "3            0.0             0.0         0.0     0.0  ...      0.0       0.0   \n",
            "4            0.0             0.0         0.0     0.0  ...      0.0       0.0   \n",
            "\n",
            "   yomomeya  yomuzizimba  yongob  yongobe  yongombe  yoo  yoontulo   zi  \n",
            "0       0.0          0.0     0.0      0.0       0.0  0.0       0.0  0.0  \n",
            "1       0.0          0.0     0.0      0.0       0.0  0.0       0.0  0.0  \n",
            "2       0.0          0.0     0.0      0.0       0.0  0.0       0.0  0.0  \n",
            "3       0.0          0.0     0.0      0.0       0.0  0.0       0.0  0.0  \n",
            "4       0.0          0.0     0.0      0.0       0.0  0.0       0.0  0.0  \n",
            "\n",
            "[5 rows x 1680 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Using Word Embeddings (TensorFlow Embedding Layer)**\n",
        "\n",
        "Why use this?\n",
        "\n",
        "Embeddings allow the model to learn contextual meaning rather than treating words as independent categories.\n",
        "\n",
        "This method is scalable and works well with deep learning architectures like CNNs and LSTMs."
      ],
      "metadata": {
        "id": "pKR7hRwUn9jP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Initialize the tokenizer\n",
        "#tokenizer = Tokenizer(num_words=vocab_size)\n",
        "#tokenizer.fit_on_texts(df[columns_to_encode].astype(str).values.flatten())\n",
        "\n",
        "# Convert text to sequences\n",
        "encoded_columns = {}\n",
        "for column in columns_to_encode:\n",
        "    sequences = tokenizer.texts_to_sequences(df[column].astype(str))\n",
        "    padded_sequences = pad_sequences(sequences, padding='post')  # Ensure same sequence length\n",
        "    encoded_columns[column] = list(padded_sequences)\n",
        "\n",
        "# Convert encoded columns into DataFrame\n",
        "encoded_df = pd.DataFrame(encoded_columns)\n",
        "\n",
        "# Display the first few rows\n",
        "print(encoded_df.head())\n",
        "\n",
        "# Vocabulary size used in the embedding layer\n",
        "embedding_vocab_size = len(tokenizer.word_index) + 1  # Add 1 for padding token\n",
        "print(f\"Embedding Vocabulary Size: {embedding_vocab_size}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ecCzNhSbknk-",
        "outputId": "f6c2a95d-4d53-43aa-f5d3-4e8847eaf7cb"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                 Oshiwambo             Aa-ndonga            Aa-kwambi  \\\n",
            "0  [307, 0, 0, 0, 0, 0, 0]  [576, 0, 0, 0, 0, 0]    [577, 0, 0, 0, 0]   \n",
            "1  [308, 0, 0, 0, 0, 0, 0]  [877, 0, 0, 0, 0, 0]    [578, 0, 0, 0, 0]   \n",
            "2    [0, 0, 0, 0, 0, 0, 0]  [140, 0, 0, 0, 0, 0]    [140, 0, 0, 0, 0]   \n",
            "3  [878, 0, 0, 0, 0, 0, 0]  [580, 0, 0, 0, 0, 0]    [309, 0, 0, 0, 0]   \n",
            "4  [880, 0, 0, 0, 0, 0, 0]  [423, 0, 0, 0, 0, 0]  [423, 881, 0, 0, 0]   \n",
            "\n",
            "         Aa-mbalanhu    Aa-kwaluudhi        Aa-kwanyama    Aa-ngandjera  \\\n",
            "0  [875, 0, 0, 0, 0]  [876, 0, 0, 0]  [307, 0, 0, 0, 0]  [577, 0, 0, 0]   \n",
            "1  [579, 0, 0, 0, 0]  [579, 0, 0, 0]  [308, 0, 0, 0, 0]  [578, 0, 0, 0]   \n",
            "2  [140, 0, 0, 0, 0]  [140, 0, 0, 0]  [140, 0, 0, 0, 0]  [140, 0, 0, 0]   \n",
            "3  [309, 0, 0, 0, 0]  [309, 0, 0, 0]  [879, 0, 0, 0, 0]  [580, 0, 0, 0]   \n",
            "4  [581, 0, 0, 0, 0]  [882, 0, 0, 0]  [581, 0, 0, 0, 0]  [423, 0, 0, 0]   \n",
            "\n",
            "             Aa-mbandja  \n",
            "0  [307, 0, 0, 0, 0, 0]  \n",
            "1  [308, 0, 0, 0, 0, 0]  \n",
            "2    [0, 0, 0, 0, 0, 0]  \n",
            "3    [0, 0, 0, 0, 0, 0]  \n",
            "4    [0, 0, 0, 0, 0, 0]  \n",
            "Embedding Vocabulary Size: 1682\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5w1mx2QMqjOF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Convert to NumPy array** and **Split into training and testing sets**"
      ],
      "metadata": {
        "id": "fgEH_xMRAZJb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ğŸ§  Summary of Why Each Step Matters:\n",
        "\n",
        "Step\t                 Purpose\n",
        "\n",
        "1. Convert to Sequences:\tConvert text into numbers the model can understand\n",
        "\n",
        "2. Pad Sequences:\tEnsure all input/output sequences are of the same length\n",
        "\n",
        "3. Convert to NumPy Arrays:\tRequired format for TensorFlow/Keras models\n",
        "\n",
        "4. Split into Train/Test Sets\tPrevent overfitting and evaluate generalization of the model"
      ],
      "metadata": {
        "id": "92ioaNtSCaZF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tokenization & Word Embeddings (Using TensorFlow)**\n",
        "Why use this?\n",
        "\n",
        "Converts text into numeric format for training CNN and LSTM models.\n",
        "\n",
        "Ensures each word is represented by dense embeddings rather than sparse one-hot vectors"
      ],
      "metadata": {
        "id": "cDdOwFqNrNwR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# Define the target labels ('Oshiwambo' column is the target)\n",
        "target_column = 'Oshiwambo'\n",
        "\n",
        "# Convert target column into numerical labels\n",
        "tokenizer_labels = Tokenizer()\n",
        "tokenizer_labels.fit_on_texts(df[target_column].astype(str))\n",
        "y = tokenizer_labels.texts_to_sequences(df[target_column].astype(str))\n",
        "\n",
        "# Pad sequences in y to ensure uniform length\n",
        "max_length = max(len(seq) for seq in y) # Find maximum sequence length in y\n",
        "y = pad_sequences(y, maxlen=max_length, padding='post', truncating='post')\n",
        "\n",
        "# Convert to NumPy array\n",
        "y = np.array(y)\n",
        "\n",
        "# Encode text columns (features)\n",
        "tokenizer = Tokenizer(num_words=5000)  # Limit vocabulary size\n",
        "tokenizer.fit_on_texts(df[columns_to_encode].astype(str).values.flatten())\n",
        "\n",
        "# Convert text to sequences\n",
        "encoded_columns = []\n",
        "for column in columns_to_encode:\n",
        "    sequences = tokenizer.texts_to_sequences(df[column].astype(str))\n",
        "    padded_sequences = pad_sequences(sequences, padding='post', maxlen=50)  # Ensure same sequence length\n",
        "    encoded_columns.append(padded_sequences)\n",
        "\n",
        "# Stack all features together\n",
        "X = np.hstack(encoded_columns)\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f\"Training Data Shape: {X_train.shape}, Testing Data Shape: {X_test.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GztK7cmo9qQO",
        "outputId": "c7802f3e-adba-4ccd-d7a6-647be1b4f04c"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Data Shape: (456, 400), Testing Data Shape: (114, 400)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Step 6: Define the CNN model**"
      ],
      "metadata": {
        "id": "jutEjapBiiVX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why use CNN?\n",
        "\n",
        "Captures short-range dependencies in text.\n",
        "\n",
        "Fast and effective for text classification."
      ],
      "metadata": {
        "id": "qf21PqR2wWUl"
      }
    },
    {
      "source": [
        "# Step 6: Define the CNN model\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, GlobalMaxPooling1D, Dense, Dropout\n",
        "\n",
        "# Reshape X (the one-hot encoded input) to fit the CNN input shape\n",
        "# CNN expects input shape as (samples, time steps, features)\n",
        "# Assuming time steps = the length of each one-hot encoded sequence\n",
        "X = X.reshape((X.shape[0], X.shape[1], 1))  # Reshape to (samples, time steps, features)\n",
        "\n",
        "# Define the CNN model\n",
        "model = Sequential()\n",
        "\n",
        "# Add a 1D convolutional layer with 64 filters and a kernel size of 5\n",
        "model.add(Conv1D(filters=64, kernel_size=5, activation='relu', input_shape=(X.shape[1], 1)))\n",
        "\n",
        "# Add a Global Max Pooling layer\n",
        "model.add(GlobalMaxPooling1D())\n",
        "\n",
        "# Add a Dense layer with 32 units and ReLU activation for feature extraction\n",
        "model.add(Dense(32, activation='relu'))\n",
        "\n",
        "# Optionally, add a Dropout layer for regularization to avoid overfitting\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "# Add the Output layer for multi-class classification\n",
        "# Use y.shape[1] to get the correct number of output units (7 in this case)\n",
        "num_classes = y.shape[1]  # Get the correct number of classes from y's shape\n",
        "model.add(Dense(num_classes, activation='softmax'))  # Adjust activation for multi-class\n",
        "\n",
        "# Compile the model using Adam optimizer and categorical crossentropy for multi-class classification\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Display the model summary to check its structure\n",
        "model.summary()\n",
        "\n",
        "# Fit the model (use the one-hot encoded `y`)\n",
        "model.fit(X, y, epochs=10, batch_size=32, validation_split=0.2)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 754
        },
        "id": "M3HQVrF0vYAh",
        "outputId": "581f3e67-9264-4095-982a-a1a15cfe90d6"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_6\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_6\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
              "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
              "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
              "â”‚ conv1d_3 (\u001b[38;5;33mConv1D\u001b[0m)                    â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m396\u001b[0m, \u001b[38;5;34m64\u001b[0m)             â”‚             \u001b[38;5;34m384\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ global_max_pooling1d_3               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  â”‚               \u001b[38;5;34m0\u001b[0m â”‚\n",
              "â”‚ (\u001b[38;5;33mGlobalMaxPooling1D\u001b[0m)                 â”‚                             â”‚                 â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense_12 (\u001b[38;5;33mDense\u001b[0m)                     â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  â”‚           \u001b[38;5;34m2,080\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dropout_6 (\u001b[38;5;33mDropout\u001b[0m)                  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  â”‚               \u001b[38;5;34m0\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense_13 (\u001b[38;5;33mDense\u001b[0m)                     â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m)                   â”‚             \u001b[38;5;34m231\u001b[0m â”‚\n",
              "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
              "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                         </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape                </span>â”ƒ<span style=\"font-weight: bold\">         Param # </span>â”ƒ\n",
              "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
              "â”‚ conv1d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                    â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">396</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ global_max_pooling1d_3               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
              "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1D</span>)                 â”‚                             â”‚                 â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dropout_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>)                   â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">231</span> â”‚\n",
              "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,695\u001b[0m (10.53 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,695</span> (10.53 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,695\u001b[0m (10.53 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,695</span> (10.53 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 30ms/step - accuracy: 0.4237 - loss: 21361.9727 - val_accuracy: 1.0000 - val_loss: 22.7129\n",
            "Epoch 2/10\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7295 - loss: 22101.3105 - val_accuracy: 1.0000 - val_loss: 36.5867\n",
            "Epoch 3/10\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7643 - loss: 31943.5605 - val_accuracy: 1.0000 - val_loss: 55.4911\n",
            "Epoch 4/10\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.7820 - loss: 48934.6055 - val_accuracy: 1.0000 - val_loss: 81.5807\n",
            "Epoch 5/10\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7811 - loss: 66719.6250 - val_accuracy: 1.0000 - val_loss: 118.0352\n",
            "Epoch 6/10\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7911 - loss: 81140.4609 - val_accuracy: 1.0000 - val_loss: 169.0833\n",
            "Epoch 7/10\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8038 - loss: 108630.1719 - val_accuracy: 1.0000 - val_loss: 234.8847\n",
            "Epoch 8/10\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8366 - loss: 167582.6562 - val_accuracy: 1.0000 - val_loss: 324.7476\n",
            "Epoch 9/10\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8347 - loss: 211057.0625 - val_accuracy: 1.0000 - val_loss: 440.5122\n",
            "Epoch 10/10\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8096 - loss: 309622.4062 - val_accuracy: 1.0000 - val_loss: 590.1003\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7a27fc261790>"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6.2 Train an LSTM Model**"
      ],
      "metadata": {
        "id": "IPUyVbdwwK0z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "LSTMs are good for learning sequential dependencies in text."
      ],
      "metadata": {
        "id": "X2qd4d01wmF8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "âœ… Why use LSTM?\n",
        "\n",
        "Captures long-range dependencies in text.\n",
        "\n",
        "Good for language modeling and context learning."
      ],
      "metadata": {
        "id": "-wslR4JLwtx1"
      }
    },
    {
      "source": [
        "#LSTM and CNN for Feature Extraction\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Input\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Define the LSTM model\n",
        "model = Sequential()\n",
        "\n",
        "# Add the Input layer with shape matching your data\n",
        "model.add(Input(shape=(X.shape[1], 1)))  # Input shape: (time steps, features)\n",
        "\n",
        "# Add an LSTM layer with 64 units\n",
        "model.add(LSTM(units=64, return_sequences=False))\n",
        "\n",
        "# Add a Dense layer with 32 units and ReLU activation for feature extraction\n",
        "model.add(Dense(32, activation='relu'))\n",
        "\n",
        "# Add a Dropout layer for regularization to avoid overfitting\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "# Add the Output layer for multi-class classification (use y_encoded's shape to get the number of classes)\n",
        "# Get the number of unique classes in your target variable (y)\n",
        "num_classes = y.shape[1]  # Assuming 'y' is one-hot encoded or has shape (samples, classes)\n",
        "model.add(Dense(num_classes, activation='softmax'))  # Multi-class classification\n",
        "\n",
        "# Compile the model using Adam optimizer and categorical crossentropy for multi-class classification\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Display the model summary to check its structure\n",
        "model.summary()\n",
        "\n",
        "# Fit the model (use the one-hot encoded `y`)\n",
        "model.fit(X, y, epochs=10, batch_size=32, validation_split=0.2)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "HzB8JhP2JH7-",
        "outputId": "246069c0-8d5a-4c1a-af9b-51835cf29336",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 670
        }
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_5\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_5\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
              "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
              "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
              "â”‚ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)                        â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  â”‚          \u001b[38;5;34m16,896\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense_10 (\u001b[38;5;33mDense\u001b[0m)                     â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  â”‚           \u001b[38;5;34m2,080\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dropout_5 (\u001b[38;5;33mDropout\u001b[0m)                  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  â”‚               \u001b[38;5;34m0\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense_11 (\u001b[38;5;33mDense\u001b[0m)                     â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m)                   â”‚             \u001b[38;5;34m231\u001b[0m â”‚\n",
              "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
              "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                         </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape                </span>â”ƒ<span style=\"font-weight: bold\">         Param # </span>â”ƒ\n",
              "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
              "â”‚ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">16,896</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>)                   â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">231</span> â”‚\n",
              "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m19,207\u001b[0m (75.03 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">19,207</span> (75.03 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m19,207\u001b[0m (75.03 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">19,207</span> (75.03 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 176ms/step - accuracy: 0.7323 - loss: 470.9318 - val_accuracy: 1.0000 - val_loss: 193.9520\n",
            "Epoch 2/10\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 148ms/step - accuracy: 0.8048 - loss: 261.4023 - val_accuracy: 1.0000 - val_loss: 2.0211\n",
            "Epoch 3/10\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 149ms/step - accuracy: 0.8189 - loss: 416.6333 - val_accuracy: 1.0000 - val_loss: 2.1389\n",
            "Epoch 4/10\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 140ms/step - accuracy: 0.8639 - loss: 508.2468 - val_accuracy: 1.0000 - val_loss: 2.9424\n",
            "Epoch 5/10\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 220ms/step - accuracy: 0.8305 - loss: 902.5732 - val_accuracy: 1.0000 - val_loss: 3.7132\n",
            "Epoch 6/10\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 140ms/step - accuracy: 0.8556 - loss: 975.3773 - val_accuracy: 1.0000 - val_loss: 4.4895\n",
            "Epoch 7/10\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 151ms/step - accuracy: 0.8425 - loss: 1399.0179 - val_accuracy: 1.0000 - val_loss: 5.2687\n",
            "Epoch 8/10\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 149ms/step - accuracy: 0.8446 - loss: 1496.0276 - val_accuracy: 1.0000 - val_loss: 6.0965\n",
            "Epoch 9/10\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 222ms/step - accuracy: 0.8627 - loss: 1754.3422 - val_accuracy: 1.0000 - val_loss: 6.9781\n",
            "Epoch 10/10\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 153ms/step - accuracy: 0.8664 - loss: 1769.7413 - val_accuracy: 1.0000 - val_loss: 7.8402\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7a27fc971790>"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train an SVM Model (Using TF-IDF)**"
      ],
      "metadata": {
        "id": "_ugzsXt5w_7G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TF-IDF + SVM is often a strong baseline for text classification."
      ],
      "metadata": {
        "id": "C6g1HngAxZm9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "âœ… Why use SVM?\n",
        "\n",
        "Works well with TF-IDF features.\n",
        "\n",
        "Strong baseline for text classification problems."
      ],
      "metadata": {
        "id": "8v8Xp4uu0Crs"
      }
    },
    {
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# Combine text from all columns\n",
        "all_text = df[columns_to_encode].astype(str).apply(lambda x: ' '.join(x), axis=1)\n",
        "\n",
        "# TF-IDF encoding\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(all_text)\n",
        "\n",
        "# Split data\n",
        "# Ensure y is 1-dimensional\n",
        "y_1d = y.argmax(axis=1)  # Assuming 'y' is one-hot encoded, get the class index\n",
        "X_train_tfidf, X_test_tfidf, y_train_tfidf, y_test_tfidf = train_test_split(\n",
        "    X_tfidf, y_1d, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "\n",
        "# Train SVM model\n",
        "svm_model = SVC(kernel='linear')\n",
        "svm_model.fit(X_train_tfidf, y_train_tfidf)  # Remove .ravel()\n",
        "\n",
        "# Evaluate\n",
        "svm_accuracy = svm_model.score(X_test_tfidf, y_test_tfidf)\n",
        "print(f\"SVM Accuracy: {svm_accuracy:.4f}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WfsJzHFTzypa",
        "outputId": "d8a1f558-a4ca-4527-d3db-2ce08259e6aa"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM Accuracy: 0.8509\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Compare Model Performance**"
      ],
      "metadata": {
        "id": "rHexQLG70ME7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After training all three models, compare them based on Accuracy, Precision, Recall, and F1-score."
      ],
      "metadata": {
        "id": "6og6ZjNg0Xmg"
      }
    },
    {
      "source": [
        "from sklearn.metrics import classification_report\n",
        "import numpy as np\n",
        "\n",
        "# Evaluate CNN model\n",
        "cnn_preds = cnn_model.predict(X_test)  # Get raw predictions\n",
        "\n",
        "# Reshape predictions to be 2D if necessary\n",
        "if cnn_preds.ndim == 3 and cnn_preds.shape[2] == 1:  # Check for extra dimension\n",
        "    cnn_preds = cnn_preds.squeeze(axis=2)  # Remove extra dimension\n",
        "\n",
        "# Evaluate LSTM model\n",
        "lstm_preds = lstm_model.predict(X_test)\n",
        "# Convert LSTM predictions to class labels (assuming you have a multi-class problem)\n",
        "lstm_preds_classes = np.argmax(lstm_preds, axis=1)\n",
        "print(\"LSTM Classification Report:\")\n",
        "# Reshape y_test to have the same number of elements as lstm_preds_classes\n",
        "print(classification_report(y_test.argmax(axis=1), lstm_preds_classes)) # Use argmax on y_test to get class labels\n",
        "\n",
        "# Evaluate SVM model\n",
        "svm_preds = svm_model.predict(X_test_tfidf)\n",
        "print(\"SVM Classification Report:\")\n",
        "print(classification_report(y_test_tfidf, svm_preds))"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hf31VwNa11Aq",
        "outputId": "fe8183c8-f13d-4538-eab7-eecb79d25c85"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
            "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 116ms/step\n",
            "LSTM Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      1.00      0.92        98\n",
            "           1       0.00      0.00      0.00        13\n",
            "           2       0.00      0.00      0.00         1\n",
            "           5       0.00      0.00      0.00         1\n",
            "           6       0.00      0.00      0.00         1\n",
            "\n",
            "    accuracy                           0.86       114\n",
            "   macro avg       0.17      0.20      0.18       114\n",
            "weighted avg       0.74      0.86      0.79       114\n",
            "\n",
            "SVM Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.99      0.92        98\n",
            "           1       0.00      0.00      0.00        13\n",
            "           2       0.00      0.00      0.00         1\n",
            "           5       0.00      0.00      0.00         1\n",
            "           6       0.00      0.00      0.00         1\n",
            "\n",
            "    accuracy                           0.85       114\n",
            "   macro avg       0.17      0.20      0.18       114\n",
            "weighted avg       0.74      0.85      0.79       114\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}